{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import section\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the local dataset\n",
    "iris=pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the first 5 rows of the dataset\n",
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigining the names of rows to strings for tab completion\n",
    "SL,SW,PL,PW='sepal_length','sepal_width','petal_length','petal_width'\n",
    "SP='species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reading the values of features into lists\n",
    "x_vals=[]\n",
    "y_vals=[]\n",
    "for index, row in iris.iterrows():\n",
    "    x=np.array([row[SL],row[SW],row[PL],row[PW]])\n",
    "    y=row[SP]\n",
    "    x_vals.append(x)\n",
    "    y_vals.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting lists into np arrays\n",
    "x_vals=np.array(x_vals)\n",
    "y_vals=np.array(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer encoding\n",
    "for i,j in enumerate(y_vals):\n",
    "    if j == 'setosa':\n",
    "        y_vals[i]=0\n",
    "    elif j == 'versicolor':\n",
    "        y_vals[i]=1\n",
    "    else:\n",
    "        y_vals[i]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertng into np arrays with int type for one hot encoding\n",
    "y_vals=np.array(y_vals,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max normalization for avoiding effects of gradient boosting\n",
    "x_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encodingof targets\n",
    "y_vals = np.eye(len(set(y_vals)))[y_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1.  0.  0.]\n",
      "1 [ 1.  0.  0.]\n",
      "2 [ 1.  0.  0.]\n",
      "3 [ 1.  0.  0.]\n",
      "4 [ 1.  0.  0.]\n",
      "5 [ 1.  0.  0.]\n",
      "6 [ 1.  0.  0.]\n",
      "7 [ 1.  0.  0.]\n",
      "8 [ 1.  0.  0.]\n",
      "9 [ 1.  0.  0.]\n",
      "10 [ 1.  0.  0.]\n",
      "11 [ 1.  0.  0.]\n",
      "12 [ 1.  0.  0.]\n",
      "13 [ 1.  0.  0.]\n",
      "14 [ 1.  0.  0.]\n",
      "15 [ 1.  0.  0.]\n",
      "16 [ 1.  0.  0.]\n",
      "17 [ 1.  0.  0.]\n",
      "18 [ 1.  0.  0.]\n",
      "19 [ 1.  0.  0.]\n",
      "20 [ 1.  0.  0.]\n",
      "21 [ 1.  0.  0.]\n",
      "22 [ 1.  0.  0.]\n",
      "23 [ 1.  0.  0.]\n",
      "24 [ 1.  0.  0.]\n",
      "25 [ 1.  0.  0.]\n",
      "26 [ 1.  0.  0.]\n",
      "27 [ 1.  0.  0.]\n",
      "28 [ 1.  0.  0.]\n",
      "29 [ 1.  0.  0.]\n",
      "30 [ 1.  0.  0.]\n",
      "31 [ 1.  0.  0.]\n",
      "32 [ 1.  0.  0.]\n",
      "33 [ 1.  0.  0.]\n",
      "34 [ 1.  0.  0.]\n",
      "35 [ 1.  0.  0.]\n",
      "36 [ 1.  0.  0.]\n",
      "37 [ 1.  0.  0.]\n",
      "38 [ 1.  0.  0.]\n",
      "39 [ 1.  0.  0.]\n",
      "40 [ 1.  0.  0.]\n",
      "41 [ 1.  0.  0.]\n",
      "42 [ 1.  0.  0.]\n",
      "43 [ 1.  0.  0.]\n",
      "44 [ 1.  0.  0.]\n",
      "45 [ 1.  0.  0.]\n",
      "46 [ 1.  0.  0.]\n",
      "47 [ 1.  0.  0.]\n",
      "48 [ 1.  0.  0.]\n",
      "49 [ 1.  0.  0.]\n",
      "50 [ 0.  1.  0.]\n",
      "51 [ 0.  1.  0.]\n",
      "52 [ 0.  1.  0.]\n",
      "53 [ 0.  1.  0.]\n",
      "54 [ 0.  1.  0.]\n",
      "55 [ 0.  1.  0.]\n",
      "56 [ 0.  1.  0.]\n",
      "57 [ 0.  1.  0.]\n",
      "58 [ 0.  1.  0.]\n",
      "59 [ 0.  1.  0.]\n",
      "60 [ 0.  1.  0.]\n",
      "61 [ 0.  1.  0.]\n",
      "62 [ 0.  1.  0.]\n",
      "63 [ 0.  1.  0.]\n",
      "64 [ 0.  1.  0.]\n",
      "65 [ 0.  1.  0.]\n",
      "66 [ 0.  1.  0.]\n",
      "67 [ 0.  1.  0.]\n",
      "68 [ 0.  1.  0.]\n",
      "69 [ 0.  1.  0.]\n",
      "70 [ 0.  1.  0.]\n",
      "71 [ 0.  1.  0.]\n",
      "72 [ 0.  1.  0.]\n",
      "73 [ 0.  1.  0.]\n",
      "74 [ 0.  1.  0.]\n",
      "75 [ 0.  1.  0.]\n",
      "76 [ 0.  1.  0.]\n",
      "77 [ 0.  1.  0.]\n",
      "78 [ 0.  1.  0.]\n",
      "79 [ 0.  1.  0.]\n",
      "80 [ 0.  1.  0.]\n",
      "81 [ 0.  1.  0.]\n",
      "82 [ 0.  1.  0.]\n",
      "83 [ 0.  1.  0.]\n",
      "84 [ 0.  1.  0.]\n",
      "85 [ 0.  1.  0.]\n",
      "86 [ 0.  1.  0.]\n",
      "87 [ 0.  1.  0.]\n",
      "88 [ 0.  1.  0.]\n",
      "89 [ 0.  1.  0.]\n",
      "90 [ 0.  1.  0.]\n",
      "91 [ 0.  1.  0.]\n",
      "92 [ 0.  1.  0.]\n",
      "93 [ 0.  1.  0.]\n",
      "94 [ 0.  1.  0.]\n",
      "95 [ 0.  1.  0.]\n",
      "96 [ 0.  1.  0.]\n",
      "97 [ 0.  1.  0.]\n",
      "98 [ 0.  1.  0.]\n",
      "99 [ 0.  1.  0.]\n",
      "100 [ 0.  0.  1.]\n",
      "101 [ 0.  0.  1.]\n",
      "102 [ 0.  0.  1.]\n",
      "103 [ 0.  0.  1.]\n",
      "104 [ 0.  0.  1.]\n",
      "105 [ 0.  0.  1.]\n",
      "106 [ 0.  0.  1.]\n",
      "107 [ 0.  0.  1.]\n",
      "108 [ 0.  0.  1.]\n",
      "109 [ 0.  0.  1.]\n",
      "110 [ 0.  0.  1.]\n",
      "111 [ 0.  0.  1.]\n",
      "112 [ 0.  0.  1.]\n",
      "113 [ 0.  0.  1.]\n",
      "114 [ 0.  0.  1.]\n",
      "115 [ 0.  0.  1.]\n",
      "116 [ 0.  0.  1.]\n",
      "117 [ 0.  0.  1.]\n",
      "118 [ 0.  0.  1.]\n",
      "119 [ 0.  0.  1.]\n",
      "120 [ 0.  0.  1.]\n",
      "121 [ 0.  0.  1.]\n",
      "122 [ 0.  0.  1.]\n",
      "123 [ 0.  0.  1.]\n",
      "124 [ 0.  0.  1.]\n",
      "125 [ 0.  0.  1.]\n",
      "126 [ 0.  0.  1.]\n",
      "127 [ 0.  0.  1.]\n",
      "128 [ 0.  0.  1.]\n",
      "129 [ 0.  0.  1.]\n",
      "130 [ 0.  0.  1.]\n",
      "131 [ 0.  0.  1.]\n",
      "132 [ 0.  0.  1.]\n",
      "133 [ 0.  0.  1.]\n",
      "134 [ 0.  0.  1.]\n",
      "135 [ 0.  0.  1.]\n",
      "136 [ 0.  0.  1.]\n",
      "137 [ 0.  0.  1.]\n",
      "138 [ 0.  0.  1.]\n",
      "139 [ 0.  0.  1.]\n",
      "140 [ 0.  0.  1.]\n",
      "141 [ 0.  0.  1.]\n",
      "142 [ 0.  0.  1.]\n",
      "143 [ 0.  0.  1.]\n",
      "144 [ 0.  0.  1.]\n",
      "145 [ 0.  0.  1.]\n",
      "146 [ 0.  0.  1.]\n",
      "147 [ 0.  0.  1.]\n",
      "148 [ 0.  0.  1.]\n",
      "149 [ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(y_vals):\n",
    "    print (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.22222222  0.625       0.06779661  0.04166667]\n",
      "1 [ 0.16666667  0.41666667  0.06779661  0.04166667]\n",
      "2 [ 0.11111111  0.5         0.05084746  0.04166667]\n",
      "3 [ 0.08333333  0.45833333  0.08474576  0.04166667]\n",
      "4 [ 0.19444444  0.66666667  0.06779661  0.04166667]\n",
      "5 [ 0.30555556  0.79166667  0.11864407  0.125     ]\n",
      "6 [ 0.08333333  0.58333333  0.06779661  0.08333333]\n",
      "7 [ 0.19444444  0.58333333  0.08474576  0.04166667]\n",
      "8 [ 0.02777778  0.375       0.06779661  0.04166667]\n",
      "9 [ 0.16666667  0.45833333  0.08474576  0.        ]\n",
      "10 [ 0.30555556  0.70833333  0.08474576  0.04166667]\n",
      "11 [ 0.13888889  0.58333333  0.10169492  0.04166667]\n",
      "12 [ 0.13888889  0.41666667  0.06779661  0.        ]\n",
      "13 [ 0.          0.41666667  0.01694915  0.        ]\n",
      "14 [ 0.41666667  0.83333333  0.03389831  0.04166667]\n",
      "15 [ 0.38888889  1.          0.08474576  0.125     ]\n",
      "16 [ 0.30555556  0.79166667  0.05084746  0.125     ]\n",
      "17 [ 0.22222222  0.625       0.06779661  0.08333333]\n",
      "18 [ 0.38888889  0.75        0.11864407  0.08333333]\n",
      "19 [ 0.22222222  0.75        0.08474576  0.08333333]\n",
      "20 [ 0.30555556  0.58333333  0.11864407  0.04166667]\n",
      "21 [ 0.22222222  0.70833333  0.08474576  0.125     ]\n",
      "22 [ 0.08333333  0.66666667  0.          0.04166667]\n",
      "23 [ 0.22222222  0.54166667  0.11864407  0.16666667]\n",
      "24 [ 0.13888889  0.58333333  0.15254237  0.04166667]\n",
      "25 [ 0.19444444  0.41666667  0.10169492  0.04166667]\n",
      "26 [ 0.19444444  0.58333333  0.10169492  0.125     ]\n",
      "27 [ 0.25        0.625       0.08474576  0.04166667]\n",
      "28 [ 0.25        0.58333333  0.06779661  0.04166667]\n",
      "29 [ 0.11111111  0.5         0.10169492  0.04166667]\n",
      "30 [ 0.13888889  0.45833333  0.10169492  0.04166667]\n",
      "31 [ 0.30555556  0.58333333  0.08474576  0.125     ]\n",
      "32 [ 0.25        0.875       0.08474576  0.        ]\n",
      "33 [ 0.33333333  0.91666667  0.06779661  0.04166667]\n",
      "34 [ 0.16666667  0.45833333  0.08474576  0.        ]\n",
      "35 [ 0.19444444  0.5         0.03389831  0.04166667]\n",
      "36 [ 0.33333333  0.625       0.05084746  0.04166667]\n",
      "37 [ 0.16666667  0.45833333  0.08474576  0.        ]\n",
      "38 [ 0.02777778  0.41666667  0.05084746  0.04166667]\n",
      "39 [ 0.22222222  0.58333333  0.08474576  0.04166667]\n",
      "40 [ 0.19444444  0.625       0.05084746  0.08333333]\n",
      "41 [ 0.05555556  0.125       0.05084746  0.08333333]\n",
      "42 [ 0.02777778  0.5         0.05084746  0.04166667]\n",
      "43 [ 0.19444444  0.625       0.10169492  0.20833333]\n",
      "44 [ 0.22222222  0.75        0.15254237  0.125     ]\n",
      "45 [ 0.13888889  0.41666667  0.06779661  0.08333333]\n",
      "46 [ 0.22222222  0.75        0.10169492  0.04166667]\n",
      "47 [ 0.08333333  0.5         0.06779661  0.04166667]\n",
      "48 [ 0.27777778  0.70833333  0.08474576  0.04166667]\n",
      "49 [ 0.19444444  0.54166667  0.06779661  0.04166667]\n",
      "50 [ 0.75        0.5         0.62711864  0.54166667]\n",
      "51 [ 0.58333333  0.5         0.59322034  0.58333333]\n",
      "52 [ 0.72222222  0.45833333  0.66101695  0.58333333]\n",
      "53 [ 0.33333333  0.125       0.50847458  0.5       ]\n",
      "54 [ 0.61111111  0.33333333  0.61016949  0.58333333]\n",
      "55 [ 0.38888889  0.33333333  0.59322034  0.5       ]\n",
      "56 [ 0.55555556  0.54166667  0.62711864  0.625     ]\n",
      "57 [ 0.16666667  0.16666667  0.38983051  0.375     ]\n",
      "58 [ 0.63888889  0.375       0.61016949  0.5       ]\n",
      "59 [ 0.25        0.29166667  0.49152542  0.54166667]\n",
      "60 [ 0.19444444  0.          0.42372881  0.375     ]\n",
      "61 [ 0.44444444  0.41666667  0.54237288  0.58333333]\n",
      "62 [ 0.47222222  0.08333333  0.50847458  0.375     ]\n",
      "63 [ 0.5         0.375       0.62711864  0.54166667]\n",
      "64 [ 0.36111111  0.375       0.44067797  0.5       ]\n",
      "65 [ 0.66666667  0.45833333  0.57627119  0.54166667]\n",
      "66 [ 0.36111111  0.41666667  0.59322034  0.58333333]\n",
      "67 [ 0.41666667  0.29166667  0.52542373  0.375     ]\n",
      "68 [ 0.52777778  0.08333333  0.59322034  0.58333333]\n",
      "69 [ 0.36111111  0.20833333  0.49152542  0.41666667]\n",
      "70 [ 0.44444444  0.5         0.6440678   0.70833333]\n",
      "71 [ 0.5         0.33333333  0.50847458  0.5       ]\n",
      "72 [ 0.55555556  0.20833333  0.66101695  0.58333333]\n",
      "73 [ 0.5         0.33333333  0.62711864  0.45833333]\n",
      "74 [ 0.58333333  0.375       0.55932203  0.5       ]\n",
      "75 [ 0.63888889  0.41666667  0.57627119  0.54166667]\n",
      "76 [ 0.69444444  0.33333333  0.6440678   0.54166667]\n",
      "77 [ 0.66666667  0.41666667  0.6779661   0.66666667]\n",
      "78 [ 0.47222222  0.375       0.59322034  0.58333333]\n",
      "79 [ 0.38888889  0.25        0.42372881  0.375     ]\n",
      "80 [ 0.33333333  0.16666667  0.47457627  0.41666667]\n",
      "81 [ 0.33333333  0.16666667  0.45762712  0.375     ]\n",
      "82 [ 0.41666667  0.29166667  0.49152542  0.45833333]\n",
      "83 [ 0.47222222  0.29166667  0.69491525  0.625     ]\n",
      "84 [ 0.30555556  0.41666667  0.59322034  0.58333333]\n",
      "85 [ 0.47222222  0.58333333  0.59322034  0.625     ]\n",
      "86 [ 0.66666667  0.45833333  0.62711864  0.58333333]\n",
      "87 [ 0.55555556  0.125       0.57627119  0.5       ]\n",
      "88 [ 0.36111111  0.41666667  0.52542373  0.5       ]\n",
      "89 [ 0.33333333  0.20833333  0.50847458  0.5       ]\n",
      "90 [ 0.33333333  0.25        0.57627119  0.45833333]\n",
      "91 [ 0.5         0.41666667  0.61016949  0.54166667]\n",
      "92 [ 0.41666667  0.25        0.50847458  0.45833333]\n",
      "93 [ 0.19444444  0.125       0.38983051  0.375     ]\n",
      "94 [ 0.36111111  0.29166667  0.54237288  0.5       ]\n",
      "95 [ 0.38888889  0.41666667  0.54237288  0.45833333]\n",
      "96 [ 0.38888889  0.375       0.54237288  0.5       ]\n",
      "97 [ 0.52777778  0.375       0.55932203  0.5       ]\n",
      "98 [ 0.22222222  0.20833333  0.33898305  0.41666667]\n",
      "99 [ 0.38888889  0.33333333  0.52542373  0.5       ]\n",
      "100 [ 0.55555556  0.54166667  0.84745763  1.        ]\n",
      "101 [ 0.41666667  0.29166667  0.69491525  0.75      ]\n",
      "102 [ 0.77777778  0.41666667  0.83050847  0.83333333]\n",
      "103 [ 0.55555556  0.375       0.77966102  0.70833333]\n",
      "104 [ 0.61111111  0.41666667  0.81355932  0.875     ]\n",
      "105 [ 0.91666667  0.41666667  0.94915254  0.83333333]\n",
      "106 [ 0.16666667  0.20833333  0.59322034  0.66666667]\n",
      "107 [ 0.83333333  0.375       0.89830508  0.70833333]\n",
      "108 [ 0.66666667  0.20833333  0.81355932  0.70833333]\n",
      "109 [ 0.80555556  0.66666667  0.86440678  1.        ]\n",
      "110 [ 0.61111111  0.5         0.69491525  0.79166667]\n",
      "111 [ 0.58333333  0.29166667  0.72881356  0.75      ]\n",
      "112 [ 0.69444444  0.41666667  0.76271186  0.83333333]\n",
      "113 [ 0.38888889  0.20833333  0.6779661   0.79166667]\n",
      "114 [ 0.41666667  0.33333333  0.69491525  0.95833333]\n",
      "115 [ 0.58333333  0.5         0.72881356  0.91666667]\n",
      "116 [ 0.61111111  0.41666667  0.76271186  0.70833333]\n",
      "117 [ 0.94444444  0.75        0.96610169  0.875     ]\n",
      "118 [ 0.94444444  0.25        1.          0.91666667]\n",
      "119 [ 0.47222222  0.08333333  0.6779661   0.58333333]\n",
      "120 [ 0.72222222  0.5         0.79661017  0.91666667]\n",
      "121 [ 0.36111111  0.33333333  0.66101695  0.79166667]\n",
      "122 [ 0.94444444  0.33333333  0.96610169  0.79166667]\n",
      "123 [ 0.55555556  0.29166667  0.66101695  0.70833333]\n",
      "124 [ 0.66666667  0.54166667  0.79661017  0.83333333]\n",
      "125 [ 0.80555556  0.5         0.84745763  0.70833333]\n",
      "126 [ 0.52777778  0.33333333  0.6440678   0.70833333]\n",
      "127 [ 0.5         0.41666667  0.66101695  0.70833333]\n",
      "128 [ 0.58333333  0.33333333  0.77966102  0.83333333]\n",
      "129 [ 0.80555556  0.41666667  0.81355932  0.625     ]\n",
      "130 [ 0.86111111  0.33333333  0.86440678  0.75      ]\n",
      "131 [ 1.          0.75        0.91525424  0.79166667]\n",
      "132 [ 0.58333333  0.33333333  0.77966102  0.875     ]\n",
      "133 [ 0.55555556  0.33333333  0.69491525  0.58333333]\n",
      "134 [ 0.5         0.25        0.77966102  0.54166667]\n",
      "135 [ 0.94444444  0.41666667  0.86440678  0.91666667]\n",
      "136 [ 0.55555556  0.58333333  0.77966102  0.95833333]\n",
      "137 [ 0.58333333  0.45833333  0.76271186  0.70833333]\n",
      "138 [ 0.47222222  0.41666667  0.6440678   0.70833333]\n",
      "139 [ 0.72222222  0.45833333  0.74576271  0.83333333]\n",
      "140 [ 0.66666667  0.45833333  0.77966102  0.95833333]\n",
      "141 [ 0.72222222  0.45833333  0.69491525  0.91666667]\n",
      "142 [ 0.41666667  0.29166667  0.69491525  0.75      ]\n",
      "143 [ 0.69444444  0.5         0.83050847  0.91666667]\n",
      "144 [ 0.66666667  0.54166667  0.79661017  1.        ]\n",
      "145 [ 0.66666667  0.41666667  0.71186441  0.91666667]\n",
      "146 [ 0.55555556  0.20833333  0.6779661   0.75      ]\n",
      "147 [ 0.61111111  0.41666667  0.71186441  0.79166667]\n",
      "148 [ 0.52777778  0.58333333  0.74576271  0.91666667]\n",
      "149 [ 0.44444444  0.41666667  0.69491525  0.70833333]\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(x_vals):\n",
    "    print (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the train and test indices from dataset(80/20 split for training and test)_\n",
    "np.random.seed(59)\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.8), replace=False)\n",
    "test_indices =np.array(list(set(range(len(x_vals))) - set(train_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into test and train according to indices obtained\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of features used by knn\n",
    "feature_number = len(x_vals_train[0])\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder is not a variable. It is a value created for the tensorflow to run computation. \n",
    "x_data_train = tf.placeholder(shape=[None, feature_number], dtype=tf.float32)\n",
    "y_data_train = tf.placeholder(shape=[None, len(y_vals[0])], dtype=tf.float32)\n",
    "x_data_test = tf.placeholder(shape=[None, feature_number], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this metrics is used to calculate the distance between the neighbours and the mean and weight of the samples is given \n",
    "#as 1/d where d is the distance\n",
    "#both l2 and l1 distance can be used.\n",
    "#l2 is the sqaure of the differnece in distance\n",
    "#l1 is the absolute value of the differnece in distance,suing l1 in this case\n",
    "distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test, 1))), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)\n",
    "top_k_label = tf.gather(y_data_train, top_k_indices)\n",
    "sum_up_predictions = tf.reduce_sum(top_k_label, axis=1)\n",
    "prediction = tf.argmax(sum_up_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the model to get the predictions\n",
    "sess = tf.Session()\n",
    "prediction_outcome = sess.run(prediction, feed_dict={x_data_train: x_vals_train,\n",
    "                               x_data_test: x_vals_test,\n",
    "                               y_data_train: y_vals_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#testing the model on the test set to obtain the accuracy\n",
    "accuracy = 0\n",
    "for pred, actual in zip(prediction_outcome, y_vals_test):\n",
    "    if pred == np.argmax(actual):\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy / len(prediction_outcome))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
